
\chapter{Introduction to Random Variables}
\section{Terminology}
As probability theory is used in quite diverse applications, terminology is not uniform and sometimes confusing. The following terms are used for non-cumulative probability distribution functions:
\begin{itemize}
* Probability mass, Probability mass function, p.m.f.: for discrete random variables.
*  Categorical distribution: for discrete random variables with a finite set of values.
*  Probability density, Probability density function, p.d.f.: most often reserved for continuous random variables.
\end{itemize}
The following terms are somewhat ambiguous as they can refer to non-cumulative or cumulative distributions, depending on authors' preferences:
\begin{itemize}
* Probability distribution function: continuous or discrete, non-cumulative or cumulative.
*  Probability function: even more ambiguous, can mean any of the above or other things.
\end{itemize}Finally,
\begin{itemize}
*  Probability distribution: sometimes the same as probability distribution function, but usually refers to the more complete assignment of probabilities to all measurable subsets of outcomes, not just to specific outcomes or ranges of outcomes.
\end{itemize}

%------------------------------------------------------------------------------------------ %


\begin{itemize}
*  The probability distribution of a random variable tells us the possible values of the variable and how
probabilities are assigned to those values.
*  The probability distribution of a discrete random variable is typically described by a list of the
values and their probabilities. Each probability is a number between 0 and 1, and the sum of the
probabilities must be equal to 1.

*  The probability distribution of a continuous random variable is typically described by a density
curve. *  The curve is defined so that the probability of any event is equal to the area under the
curve for the values that make up the event, and the total area under the curve is equal to 1. *  One
example of a continuous probability distribution is the normal distribution.

*  We use the term parameter to refer to a number that describes some characteristic of a population. *   We
rarely know the true parameters of a population, and instead estimate them with statistics. *  Statistics
are numbers that we can calculate purely from a sample. *  The value of a statistic is random, and will
depend on the specific observations included in the sample.

*  The law of large numbers states that if we draw a bunch of numbers from a population with mean ยน,
we can expect the mean of the numbers $\bar{y}$ to be closer to $\mu$ as we increase the number we draw. This
means that we can estimate the mean of a population by taking the average of a set of observations
drawn from that population.
\end{itemize}
#########################################################################################################

\section{Joint probability tables}
A joint probability table is a table in which all possible events (or outcomes) for one variable are listed as
row headings, all possible events for a second variable are listed as column headings, and the value entered in
each cell of the table is the probability of each joint occurrence. 

Often, the probabilities in such a table are based
on observed frequencies of occurrence for the various joint events. The table
of joint-occurrence frequencies which can serve as the basis for constructing a joint probability table is called a
contingency table.

\begin{enumerate}
*  A pair of dice is thrown. Let X denote the minimum of the two numbers which occur.
Find the distributions and expected value of X.

*  A fair coin is tossed four times.
Let X denote the longest string of heads.
Find the distribution and expectation of X.

*  A fair coin is tossed until a head or five tails occurs.
Find the expected number E of tosses of the coin.


*  A coin is weighted so that P(H) = 0.75 and P(T ) = 0.25
*  The coin is tossed three times. Let X denote the number of
heads that appear.
\begin{itemize}
*  (a) Find the distribution f of X.
*  (b) Find the expectation E(X).
\end{itemize}
\end{enumerate}

%======================================================================================= %

\section{Graph of a Probability Distribution}

A probability distribution can be graphed, and sometimes this helps to show us features of the distribution that were not apparent from just reading the list of probabilities. The random variable is plotted along the x-axis, and the corresponding probability is plotted along the y - axis.

\begin{itemize}
*  For a discrete random variable, we will have a histogram
*  For a continuous random variable, we will have the inside of a smooth curve
\end{itemize}

The rules of probability are still in effect, and they manifest themselves in a few ways. Since probabilities are greater than or equal to zero, the graph of a probability distribution must have y-coordinates that are nonnegative. Another feature of probabilities, namely that one is the maximum that the probability of an event can be, shows up in another way.

\[ \mbox{Area} = \mbox{Probability} \]




\end{document}
