\documentclass[]{article}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
%opening

\begin{document}

%---------------------------------------%
\section{Question 5 (Sample Variant 1)[25 marks]}

\begin{itemize}
\item[(a)] \textbf{\textit{Huffman Coding (5 Marks)}}\\
A discrete memoryless source $X$ has five symbols $\{x_1,x_2,x_3,x_4,x_5\}$ with probabilities $P(x_1) = 0.45$ , $P(x_2) = 0.20$, $P(x_3) = 0.16$, $P(x_4) = 0.14$ and $P(x_5) = 0.05$.

\begin{itemize}
\item[(i)] (5 Marks) Construct a Huffman code for X.
%\item[(ii)] (3 Marks) Calculate the efficiency of the code.
%\item[(iii)] (1 marks) Calculate the redundancy of the code.
\end{itemize}

\item[(b)] \textbf{\textit{Rate of Information (6 Marks)}}\\
A Computer monitor consists of about $2 \times 10^4$ picture elements (symbols) and 8
different brightness levels.\\
\newline
Pictures are repeated at a rate of 16 per second. All picture elements
are assumed to be independent, and all levels have equal likelihood of occurrence. 
\begin{itemize}
\item[(i)](6 Marks) Calculate the
average rate of information conveyed by this TV picture source.
\end{itemize}


\item[(c)] \textbf{\textit{Communication Channels (14 Marks)}}\\
The input source to a noisy communication channel is a random variable X over the
four symbols $\{a, b, c, d\}$. The output from this channel is a random variable Y over these same
four symbols. \\
\noindent 
The joint distribution of these two random variables is as follows:\\ 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
y=a & 0.125	&	0.03125	&	0	&	0.015625	\\ \hline
y=b & 0	&	0.1875	&	0.125	&	0	\\ \hline
y=c & 0	&	0.015625	&	0.1875	&	0	\\ \hline
y=d & 0.0625	&	0	&	0	&	0.25	\\ \hline
\end{tabular}
\end{center}

\begin{itemize}
\item[(i)] (2 Marks) Write down the marginal distribution for $X$ and compute the marginal entropy $H(X)$.
\item[(ii)] (2 Marks) Write down the marginal distribution for $Y$ and compute the marginal entropy $H(Y )$.
\item[(iii)] (2 Marks) What is the joint entropy $H(X, Y ) $ of the two random variables?
\item[(iv)] (4 marks) What is the conditional entropy $H(Y|X)$?
\item[(v)] (2 marks) What is the conditional entropy $H(X|Y)$?
\item[(vi)] (2 marks) What is the mutual information $I(X;Y)$ between the two random variables?
\end{itemize}
\end{itemize}
\end{document}